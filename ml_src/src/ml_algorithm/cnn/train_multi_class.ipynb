{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "train cnn mode for sentiment classification on yelp data set\n",
    "'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "\n",
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = []\n",
    "    for review in data[\"text\"]:\n",
    "        review_words = Word2VecUtility.review_to_wordlist(\n",
    "            review, remove_stopwords=True)\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary.append(word)\n",
    "    volcabulary = set(volcabulary)\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    word2index = {word: i for i, word in enumerate(volcabulary)}\n",
    "    # use w in volcabulary to limit index within max_words\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    reviews_words_index = [[i if (i < max_words) else oov for i in index] for index in reviews_words_index]\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index\n",
    "\n",
    "def vectorize_labels(labels, nums):\n",
    "    labels = np.asarray(labels, dtype='int32')\n",
    "    length = len(labels)\n",
    "    Y = np.zeros((length, nums))\n",
    "    for i in range(length):\n",
    "        Y[i, (labels[i]-1)] = 1.\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 50)\n",
      "(200000, 50)\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# data processing para\n",
    "max_words = 5000\n",
    "max_length = 50\n",
    "\n",
    "# model training parameters\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "\n",
    "(reviews_words_index, labels) = pickle.load(open(\"399850by50reviews_words_index.pkl\", 'rb'))\n",
    "\n",
    "index = np.arange(reviews_words_index.shape[0])\n",
    "train_index, valid_index = train_test_split(\n",
    "    index, train_size=0.8, random_state=520)\n",
    "\n",
    "labels = vectorize_labels(labels, 5)\n",
    "train_data = reviews_words_index[train_index]\n",
    "valid_data = reviews_words_index[valid_index]\n",
    "train_labels = labels[train_index]\n",
    "valid_labels = labels[valid_index]\n",
    "print train_data.shape\n",
    "print valid_data.shape\n",
    "print train_labels[:10]\n",
    "\n",
    "del(labels, train_index, valid_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/models.py:571: UserWarning: \"class_mode\" argument is deprecated, please remove it.\n",
      "  warnings.warn('\"class_mode\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/2\n",
      "800000/800000 [==============================] - 798s - loss: 0.6258 - acc: 0.6728 - val_loss: 0.6215 - val_acc: 0.6721\n",
      "Epoch 2/2\n",
      "800000/800000 [==============================] - 771s - loss: 0.6223 - acc: 0.6729 - val_loss: 0.6233 - val_acc: 0.6721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129d10fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print \"start training model...\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                    input_length=max_length))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "# filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use standard max pooling (halving the output of the previous layer):\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto 5 unit output layer, and activate it with softmax:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              class_mode='categorical',metrics=[\"accuracy\"])\n",
    "model.fit(train_data, train_labels, batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch, validation_data=(valid_data, valid_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
